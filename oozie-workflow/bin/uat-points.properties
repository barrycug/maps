# environment the application is running
# c4yarn.gbif-uat.org, but the alias isn't accepted
hadoop.jobtracker=c4master2-vh.gbif.org:8032
hdfs.namenode=hdfs://ha-nn
oozie.url=http://c4oozie.gbif-uat.org:11000/oozie

# Uses oozies shared lib and the /lib folder in the workflow
oozie.use.system.libpath=true

# hdfs because we do things like chown hbase:hbase
user.name=hdfs

# location of the workflow and jars
#oozie.wf.application.path=hdfs://ha-nn/maps-backfill-workflow
oozie.coord.application.path=hdfs://ha-nn/maps-backfill-workflow/coordinator-points.xml
oozie.libpath=hdfs://ha-nn/maps-backfill-workflow/lib/

# fine tune the spark submission
gbif.map.spark.opts=--driver-memory 2G --executor-memory 6G --num-executors 10 --executor-cores 5 --conf spark.yarn.executor.memoryOverhead=2048

# the name of the file for the configuration
gbif.map.spark.conf=uat.yml

# ZK Quorum for the HBase cluster
gbif.map.zk.quorum=c4zk1.gbif-uat.org:2181,c4zk2.gbif-uat.org:2181,c4zk3.gbif-uat.org:2181

# Source HBase occurrence table to snapshot for the input
gbif.map.sourceTable=uat_occurrence

# The target table is finalised as prefix_mode_timestamp (e.g. dev_maps_tiles_201707_01_1315
gbif.map.targetTablePrefix=uat_maps

# The target directory for the HFiles
gbif.map.targetDirectory=hdfs://ha-nn/tmp/uat_maps

# Whether to run the point or the tile backfill
gbif.map.mode=points
#gbif.map.mode=tiles

# The modulus controls the number of regions in the target table (sensible defaults are 5-10 for dev and 100 for prod)
gbif.map.keySaltModulus=100

# The ZK Path where the table names are located
gbif.map.zk.metadataPath=/uat_maps/meta
